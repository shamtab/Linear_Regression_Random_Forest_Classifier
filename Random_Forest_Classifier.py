# -*- coding: utf-8 -*-
"""Big_Data_HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ct6ic69X3X05sV5JNckt-C-lt-Olw7ID
"""

#P1

#import pandas
import pandas as pd

#import numpy
import numpy as np

names = ['mpg','cylinders','displacement','horsepower','weight','acceleration','model_year','origin','car_name']

#read autompg dataset from url
data = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data", names = names, sep = '\s+', na_values = '?')

#print first 5 rows of the dataframe
data.head()

#shape of the dataframe
data.shape

#Delete the “car_name” column using .drop() method
data_new = data.drop(columns='car_name')

#Print out a concise summary of the new DataFrame using .info()
data_new.info()

# Count total NaN at each column in a DataFrame 
print(" \nCount total NaN at each column in DataFrame : \n\n", 
      data_new.isnull().sum())

#replace missing values of the column with mean value
data_new['horsepower'].fillna(int(data_new['horsepower'].mean()), inplace=True)

#Print out a concise summary of the new DataFrame using .info()
data_new.info()

# Recheck total NaN at each column in a DataFrame 
print(" \nRechecking Count total NaN at each column in DataFrame : \n\n", 
      data_new.isnull().sum())

data_new_onehotencoding = pd.get_dummies(data_new.origin, prefix='Origin')
print(data_new_onehotencoding.head())
data_new_onehotencoding.info()

#Select Predictor columns
X = data_new[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year']]

#Select target column
y = data_new['mpg']

from sklearn import linear_model
from sklearn.model_selection import train_test_split
#Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)
print(X_train.shape)
print(X_test.shape)

#Train linear regression model
regr = linear_model.LinearRegression()
regr.fit(X_train, y_train)

#Print coefficients
list(zip(names[1:8], regr.coef_))

#Predict model on test set
#Mean Squared error on the testing set
preds_ = regr.predict(X_test)
mse_ = np.mean((preds_ - y_test) ** 2)
rsq_ = regr.score(X_test, y_test)

print("Mean Squared Error: %.4f \n R-squared: %.4f" % (mse_,rsq_))

#P2

## Import required packages

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
#from sklearn.linear_model import SGDClassifier
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score

## Read wine dataset

wine = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep = ';')

#print first five rows of the dataset
print(wine.head())

#print concise summary of the dataset
print(wine.info())

#Seperate the dataset as response variable and feature variables

X = wine.drop('quality', axis = 1)
y = wine['quality']

#Split dataset into training and test data 80/20 split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

#Applying Standard scaling
sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

#Learn classification model with randomforestclassifier

classifier = RandomForestClassifier(n_estimators=300, random_state=0)

from sklearn.model_selection import cross_val_score
all_accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=5)

print(all_accuracies)

#average values
print(all_accuracies.mean())

#standard deviation value
print(all_accuracies.std())

#GridSearchCV

grid_param = {
    'n_estimators': [100, 300, 500, 800, 1000],
    }

#Use 5-fold cross-validation during GridSearchCV

from sklearn.model_selection import GridSearchCV
gd_sr = GridSearchCV(estimator=classifier,
                     param_grid=grid_param,
                     scoring='accuracy',
                     cv=5,
                     n_jobs=-1)

gd_sr.fit(X_train, y_train)

best_parameters = gd_sr.best_params_
print(best_parameters)

best_result = gd_sr.best_score_
print(best_result)